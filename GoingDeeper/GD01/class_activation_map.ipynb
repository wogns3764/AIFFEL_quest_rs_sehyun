{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b97dda3",
   "metadata": {},
   "source": [
    "### 프로젝트 변경점\n",
    "- 사전 학습된 ResNet50을 사용(기존 v1 > v2로 변경)\n",
    "- optimizer 변경(기존 SGD > AdamW로 변경)\n",
    "\n",
    "### 프로젝트 결과 분석\n",
    "- 가정: 모델이 충분히 학습이 되었다는 전제하에 CAM과 Grad GAM의 결과를 분석하기\n",
    "- CAM과 Grad CAM의 결과는 동일하게 나타나는 것을 볼 수 있다.\n",
    "    -  이유: 현재 ResNet 구조를 보면, 마지막 Layer에서 AVG(1,1,c)를 사용하고 있기 때문에 GAP을 사용한 것과 같은 결과가 나온다.  \n",
    "    만약 ResNet의 구조를 변경한다면, CAM과 Grad CAM에서 다른 결과를 확인할 수 있을 것이다. 하지만 CAM과 Grad GAM의 결과를 Ablation Study 관점에서 비교하기 어렵다는 문제가 있다.\n",
    "\n",
    "- 여기서 주의깊게 살펴볼 것은 Grad CAM이다.  \n",
    "    - 입력층과 가까운 저차원 레이어는 에지나 텍스처 같은 국소적인 기하학적 특징을 추출하며, 이는 히트맵과 바운딩 박스를 통해 모델이 이미지의 전반적인 기초 윤곽을 탐색하고 있음을 보여준다.  \n",
    "    - 반대로 출력층에 가까운 레이어(고차원)는 레이어를 거치며 극대화된 Receptive Field을 통해 이미지 전체의 문맥을 통합적으로 수용하며, 고차원의 의미론적 추론을 바탕으로 압축된 특징 맵 속에서 객체의 본질적 형상에만 정보를 집중시켜 인식의 범위를 정교하게 수렴시킨다.\n",
    "\n",
    "- 사전 학습된 ResNet 모델의 학습 과정에서 훈련 손실($L_{train}$)과 정확도는 최적점으로 점근적 수렴을 보이나, 검증(Validation) 지표는 특정 시점 이후 최적화 정체기(Optimization Plateau)에 진입한 것을 알 수 있다.  \n",
    "\n",
    "    특히 AdamW의 가중치 감쇠(Weight Decay) 기법이 검증 손실의 급격한 발산을 억제하여 전형적인 과대적합 양상은 나타나지 않지만, 학습이 지속될수록 모델이 객체의 본질적 형상이 아닌 훈련 데이터 특유의 비본질적 노이즈를 암기하게 된다. 이로 인해 수치상의 안정에도 불구하고 실질적인 객체 변별력이 약화되는 일반화 성능 저하가 발생하며, 결과적으로 에포크가 증가할수록 타겟 객체에 대한 탐지 정밀도가 하락하는 결과가 발생한 것을 알 수 있다.\n",
    "\n",
    "\n",
    "### 회고 \n",
    "입력층에 인접한 저차원 레이어에서 에지나 텍스처 같은 국소적 특징을 추출하고, 이를 계층적으로 결합하여 고차원 레이어에서 객체의 공간적 위상과 고차원적 의미론적 추론을 수행하는 과정이 인상적이었다.  \n",
    "특히, 초기 레이어의 시각적 파편들이 여러 단계의 필터를 거치며 Receptive Field를 확장하고, 객체를 인식해나가는 과정에서 인공신경망이 단순한 연산을 넘어 인간의 추론 방식을 공학적으로 재현하고 있음을 알게 되었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34bc2684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install opencv-python, numpy, torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21dcd0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget http://vision.stanford.edu/aditya86/ImageNetDogs/images.tar\n",
    "# !wget http://vision.stanford.edu/aditya86/ImageNetDogs/annotation.tar\n",
    "# !wget http://vision.stanford.edu/aditya86/ImageNetDogs/lists.tar\n",
    "# !tar -xf images.tar -C ./\n",
    "# !tar -xf annotation.tar -C ./\n",
    "# !tar -xf lists.tar -C ./\n",
    "\n",
    "# print(\"데이터 다운로드 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33559f5a",
   "metadata": {},
   "source": [
    "## 프로젝트: CAM을 만들고 평가해 보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a318138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0+cu128\n",
      "2.4.2\n",
      "4.13.0\n",
      "12.1.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import cv2\n",
    "import PIL\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import ResNet50_Weights\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n",
    "\n",
    "print(torch.__version__)\n",
    "print(np.__version__)\n",
    "print(cv2.__version__)\n",
    "print(PIL.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f415d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 10\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ccce29",
   "metadata": {},
   "source": [
    "#### 데이터 분류\n",
    "- stanford_dogs 폴더가 없을 때 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06f4b966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 원본 데이터 경로 (압축 해제 후 폴더)\n",
    "# images_dir = './Images'  # 이미지들이 위치한 폴더\n",
    "# mat_dir = './' # 프로젝트 루트 디렉토리\n",
    "\n",
    "# # .mat 파일 경로 (train_list.mat와 test_list.mat가 각각 같은 구조라고 가정)\n",
    "# train_mat_path = os.path.join(mat_dir, 'train_list.mat')\n",
    "# test_mat_path = os.path.join(mat_dir, 'test_list.mat')\n",
    "\n",
    "# # .mat 파일 로드\n",
    "# train_mat = sio.loadmat(train_mat_path)\n",
    "# test_mat = sio.loadmat(test_mat_path)\n",
    "\n",
    "# # train_mat와 test_mat 내부에 'file_list'와 'labels' 등이 있음\n",
    "# train_file_list = train_mat['file_list']\n",
    "# train_labels = train_mat['labels'].squeeze()  # (N,)\n",
    "# test_file_list = test_mat['file_list']\n",
    "# test_labels = test_mat['labels'].squeeze()\n",
    "\n",
    "# # 최종적으로 ImageFolder 구조로 재구성할 대상 폴더 생성 (예: stanford_dogs/train, stanford_dogs/test)\n",
    "# base_dir = 'stanford_dogs'\n",
    "# train_dir = os.path.join(base_dir, 'train')\n",
    "# test_dir = os.path.join(base_dir, 'test')\n",
    "\n",
    "# os.makedirs(train_dir, exist_ok=True)\n",
    "# os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "# def process_mat_file(file_list_array, split_dir):\n",
    "#     \"\"\"\n",
    "#     .mat 파일에서 로드한 file_list_array를 사용해,\n",
    "#     split_dir(예: train 혹은 test)에 클래스별 폴더를 생성하고 이미지를 복사합니다.\n",
    "#     \"\"\"\n",
    "#     num_files = file_list_array.shape[0]\n",
    "#     for idx in range(num_files):\n",
    "#         # file_list_array[idx]는 보통 array([<파일경로>]) 형태입니다.\n",
    "#         # 따라서, array([<파일경로>]).item()을 사용하면 실제 문자열을 얻을 수 있습니다.\n",
    "#         file_path = file_list_array[idx][0].item()\n",
    "\n",
    "#         # 혹시 bytes 타입이면 문자열로 디코딩\n",
    "#         if isinstance(file_path, bytes):\n",
    "#             file_path = file_path.decode('utf-8')\n",
    "\n",
    "#         # 파일 경로 예시: 'n02116738-African_hunting_dog/n02116738_2988.jpg'\n",
    "#         # 클래스 이름은 파일 경로의 최상위 폴더명 (예: 'n02116738-African_hunting_dog')\n",
    "#         class_folder = file_path.split('/')[0]\n",
    "\n",
    "#         # 대상 클래스 폴더 생성\n",
    "#         dest_folder = os.path.join(split_dir, class_folder)\n",
    "#         os.makedirs(dest_folder, exist_ok=True)\n",
    "\n",
    "#         # 원본 이미지 경로: Images 폴더 아래에 file_path 위치\n",
    "#         src_path = os.path.join(images_dir, file_path)\n",
    "#         # 대상 이미지 경로: dest_folder 아래에 원본 파일명 그대로 복사\n",
    "#         dest_path = os.path.join(dest_folder, os.path.basename(file_path))\n",
    "\n",
    "#         # 파일 존재 여부 확인 후 복사\n",
    "#         if os.path.exists(src_path):\n",
    "#             shutil.copy(src_path, dest_path)\n",
    "#         else:\n",
    "#             print(f\"File not found: {src_path}\")\n",
    "\n",
    "# print(\"Processing train set...\")\n",
    "# process_mat_file(train_file_list, train_dir)\n",
    "# print(\"Processing test set...\")\n",
    "# process_mat_file(test_file_list, test_dir)\n",
    "\n",
    "# print(\"Dataset reorganization complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "205409e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StanfordDogsDatasetWithBBox(datasets.ImageFolder):\n",
    "    def __init__(self, root, annotation_root, transform=None):\n",
    "        super().__init__(root, transform=transform)\n",
    "        self.annotation_root = annotation_root  # 예: '/content/Annotation'\n",
    "        self.new_size = (224, 224)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image, label = super().__getitem__(index)\n",
    "        path, _ = self.samples[index]\n",
    "\n",
    "        rel_path = os.path.relpath(path, self.root)\n",
    "        annot_filename = os.path.splitext(os.path.basename(rel_path))[0]\n",
    "        annot_folder = os.path.dirname(rel_path)\n",
    "        annot_path = os.path.join(self.annotation_root, annot_folder, annot_filename)\n",
    "\n",
    "        # bbox 기본값 (예: [ymin, xmin, ymax, xmax])\n",
    "        bbox = [0.0, 0.0, 0.0, 0.0]\n",
    "\n",
    "        if os.path.exists(annot_path):\n",
    "            try:\n",
    "                # 확장자가 없지만 XML 형식의 파일이라고 가정하고 파싱\n",
    "                tree = ET.parse(annot_path)\n",
    "                root_xml = tree.getroot()\n",
    "                # 첫 번째 object 태그에서 bndbox 정보를 읽음\n",
    "                obj = root_xml.find('object')\n",
    "                if obj is not None:\n",
    "                    bndbox = obj.find('bndbox')\n",
    "                    if bndbox is not None:\n",
    "                        xmin = float(bndbox.find('xmin').text)\n",
    "                        ymin = float(bndbox.find('ymin').text)\n",
    "                        xmax = float(bndbox.find('xmax').text)\n",
    "                        ymax = float(bndbox.find('ymax').text)\n",
    "                        # XML 내 <size> 태그에서 원본 이미지 크기 획득\n",
    "                        size = root_xml.find('size')\n",
    "                        w = float(size.find('width').text)\n",
    "                        h = float(size.find('height').text)\n",
    "                        new_h, new_w = self.new_size\n",
    "                        # bbox 좌표 순서: [ymin, xmin, ymax, xmax]\n",
    "                        bbox = [xmin * (new_w / w), ymin * (new_h / h),\n",
    "                                xmax * (new_w / w), ymax * (new_h / h)]\n",
    "                    else:\n",
    "                        print(f\"bndbox 태그를 찾을 수 없습니다: {annot_path}\")\n",
    "                else:\n",
    "                    print(f\"object 태그를 찾을 수 없습니다: {annot_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing {annot_path}: {e}\")\n",
    "        else:\n",
    "            print(f\"Annotation file not found: {annot_path}\")\n",
    "\n",
    "        return image, label, bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43899b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unnormalize(img_tensor):\n",
    "    \"\"\"\n",
    "    img_tensor: [C, H, W] 텐서 (전처리 상태, 예: normalization 적용됨)\n",
    "    ImageNet 평균 및 표준편차를 이용하여 복원 (RGB 순서)\n",
    "    \"\"\"\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std  = np.array([0.229, 0.224, 0.225])\n",
    "    img = img_tensor.cpu().numpy().transpose(1, 2, 0)\n",
    "    img = std * img + mean\n",
    "    img = np.clip(img, 0, 1)\n",
    "    img = np.uint8(255 * img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2371ec7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bbox(cam, threshold=0.01):\n",
    "    coords = np.argwhere(cam > threshold)\n",
    "    if coords.size == 0:\n",
    "        return None\n",
    "    # np.argwhere의 결과는 (row, col) 즉, (y, x) 순서입니다.\n",
    "    y_min, x_min = coords.min(axis=0)\n",
    "    y_max, x_max = coords.max(axis=0)\n",
    "    return (x_min, y_min, x_max, y_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "196438f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_bbox_on_image(img, bbox, box_color=(255, 0, 0), thickness=2):\n",
    "    # 원본 이미지 복사\n",
    "    img_with_bbox = img.copy()\n",
    "    if bbox is not None:\n",
    "        x_min, y_min, x_max, y_max = bbox\n",
    "        cv2.rectangle(img_with_bbox, (x_min, y_min), (x_max, y_max), box_color, thickness)\n",
    "    else:\n",
    "        print(\"활성화된 영역이 없습니다.\")\n",
    "    return img_with_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b4c633d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_both_bbox_on_image(img, bbox, ground_truth, thickness=2):\n",
    "    # 원본 이미지 복사\n",
    "    img_with_bbox = img.copy()\n",
    "\n",
    "    x_min, y_min, x_max, y_max = bbox\n",
    "    cv2.rectangle(img_with_bbox, (x_min, y_min), (x_max, y_max), (255, 0, 0), thickness)\n",
    "\n",
    "    x_min_t, y_min_t, x_max_t, y_max_t = ground_truth\n",
    "    cv2.rectangle(img_with_bbox, (x_min_t, y_min_t), (x_max_t, y_max_t), (0, 255, 0), thickness)\n",
    "\n",
    "    return img_with_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6eb14350",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iou(gt_bbox, pred_bbox):\n",
    "    iou = None\n",
    "    # TODO: get iou between two bbox\n",
    "    # bbox 중 하나라도 None이면 IoU 0.0\n",
    "    if gt_bbox is None or  pred_bbox is None:\n",
    "        return 0.0\n",
    "\n",
    "    x_min1, y_min1, x_max1, y_max1 = gt_bbox\n",
    "    x_min2, y_min2, x_max2, y_max2 =  pred_bbox\n",
    "\n",
    "    # 두 bbox의 교집합 영역 좌표 계산\n",
    "    x_min_inter = max(x_min1, x_min2)\n",
    "    y_min_inter = max(y_min1, y_min2)\n",
    "    x_max_inter = min(x_max1, x_max2)\n",
    "    y_max_inter = min(y_max1, y_max2)\n",
    "\n",
    "    # 교집합의 너비와 높이 (음수가 되지 않도록)\n",
    "    inter_width = max(0, x_max_inter - x_min_inter)\n",
    "    inter_height = max(0, y_max_inter - y_min_inter)\n",
    "    inter_area = inter_width * inter_height\n",
    "\n",
    "    # 각 bbox의 면적 계산\n",
    "    area1 = (x_max1 - x_min1) * (y_max1 - y_min1)\n",
    "    area2 = (x_max2 - x_min2) * (y_max2 - y_min2)\n",
    "\n",
    "    # 합집합 면적: A ∪ B = A + B - A ∩ B\n",
    "    union_area = area1 + area2 - inter_area\n",
    "    if union_area <= 0:\n",
    "        return 0.0\n",
    "\n",
    "    iou = inter_area / union_area\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07197332",
   "metadata": {},
   "source": [
    "### CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d41f03b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cam(model, item):\n",
    "    cam_image = None\n",
    "    # TODO: generate cam image\n",
    "    model.eval()\n",
    "    features = []\n",
    "\n",
    "    def hook(module, input, output):\n",
    "        features.append(output.detach())\n",
    "    hook_handle = model.layer4.register_forward_hook(hook)\n",
    "\n",
    "    output = model(item)\n",
    "    hook_handle.remove()\n",
    "\n",
    "    pred_class = output.argmax(dim=1).item()\n",
    "\n",
    "    fc_weights = model.fc.weight.data.to(item.device)\n",
    "\n",
    "    fmap = features[0][0]\n",
    "    cam_image = torch.zeros(fmap.shape[1:], dtype=torch.float32, device=item.device)\n",
    "\n",
    "    for i, w in enumerate(fc_weights[pred_class]):\n",
    "        cam_image += w * fmap[i, :, :]\n",
    "    cam_image = cam_image.cpu().numpy()\n",
    "\n",
    "    cam_image = np.maximum(cam_image, 0)\n",
    "    cam_image = (cam_image - np.min(cam_image)) / (np.max(cam_image) - np.min(cam_image) + 1e-8)\n",
    "\n",
    "    return cam_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23a4c47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cam(models, image, ground_bbox, item, start_epochs=1, step_epochs=2):\n",
    "    num_epochs = len(models)\n",
    "    rows = 3\n",
    "    cols = num_epochs + 1 \n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(4 * cols, 4 * rows))\n",
    "    \n",
    "    orig_img = unnormalize(image)\n",
    "    ground_truth = [int(x) for x in ground_bbox]\n",
    "\n",
    "    axes[0, 0].imshow(orig_img)\n",
    "    axes[0, 0].set_title(\"Original Image\")\n",
    "    axes[0, 0].axis(\"off\")\n",
    "    \n",
    "    axes[1, 0].imshow(orig_img)\n",
    "    axes[1, 0].set_title(\"Original (For Overlay Ref)\")\n",
    "    axes[1, 0].axis(\"off\")\n",
    "    \n",
    "    gt_only_img = visualize_bbox_on_image(orig_img.copy(), ground_truth) if 'visualize_bbox_on_image' in globals() else orig_img\n",
    "    axes[2, 0].imshow(gt_only_img)\n",
    "    axes[2, 0].set_title(\"Ground Truth BBox\")\n",
    "    axes[2, 0].axis(\"off\")\n",
    "\n",
    "    for i, model in enumerate(models):\n",
    "        col_idx = i + 1\n",
    "        epoch_num = start_epochs + (i * step_epochs)\n",
    "        \n",
    "        cam = generate_cam(model, item)\n",
    "        cam_resized = cv2.resize(cam, (orig_img.shape[1], orig_img.shape[0]))\n",
    "        cam_bbox = get_bbox(cam_resized, threshold=0.5)\n",
    "        \n",
    "        axes[0, col_idx].imshow(cam)\n",
    "        axes[0, col_idx].set_title(f\"Epoch {epoch_num} CAM\")\n",
    "        axes[0, col_idx].axis(\"off\")\n",
    "        \n",
    "        axes[1, col_idx].imshow(orig_img)\n",
    "        axes[1, col_idx].imshow(cam_resized, cmap='jet', alpha=0.5) \n",
    "        axes[1, col_idx].set_title(f\"Epoch {epoch_num} Overlay\")\n",
    "        axes[1, col_idx].axis(\"off\")\n",
    "        \n",
    "        iou_score = get_iou(cam_bbox, ground_truth) \n",
    "        cam_img_bbox = visualize_both_bbox_on_image(orig_img.copy(), cam_bbox, ground_truth)\n",
    "        \n",
    "        axes[2, col_idx].imshow(cam_img_bbox)\n",
    "        title_text = f\"Epoch {epoch_num} IOU: {iou_score:.4f}\" if iou_score is not None else f\"Epoch {epoch_num} Compare\"\n",
    "        axes[2, col_idx].set_title(title_text)\n",
    "        axes[2, col_idx].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce3d7f5",
   "metadata": {},
   "source": [
    "### grad cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31d65fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_grad_cam(model,item, activation_layer= \"layer4\"):\n",
    "    # TODO: generate grad_cam_image\n",
    "    model.eval()\n",
    "    features = {}\n",
    "    gradients = {}\n",
    "\n",
    "    # forward hook: 대상 레이어의 출력을 저장\n",
    "    def forward_hook(module, input, output):\n",
    "        features['value'] = output.detach()\n",
    "\n",
    "    # backward hook: 대상 레이어의 gradient를 저장\n",
    "    def backward_hook(module, grad_in, grad_out):\n",
    "        gradients['value'] = grad_out[0].detach()\n",
    "\n",
    "    # 모델 내에서 이름이  activation_layer과 일치하는 레이어 검색\n",
    "    target_layer = dict(model.named_modules()).get( activation_layer, None)\n",
    "    if target_layer is None:\n",
    "        raise ValueError(f\"Layer '{activation_layer}' not found in the model.\")\n",
    "    \n",
    "    # hook 등록\n",
    "    forward_handle = target_layer.register_forward_hook(forward_hook)\n",
    "    backward_handle = target_layer.register_backward_hook(backward_hook)\n",
    "\n",
    "    # 순전파 실행\n",
    "    output = model(item)\n",
    "    pred_class = output.argmax(dim=1).item()\n",
    "\n",
    "    # 예측 클래스에 대해 backward 수행\n",
    "    model.zero_grad()\n",
    "    score = output[0, pred_class]\n",
    "    score.backward()\n",
    "\n",
    "    # hook 제거\n",
    "    forward_handle.remove()\n",
    "    backward_handle.remove()\n",
    "\n",
    "    # 저장된 feature map과 gradient 추출 (shape: [C, H, W])\n",
    "    fmap = features['value'][0]\n",
    "    grads = gradients['value'][0]\n",
    "\n",
    "    # 각 채널에 대해 gradient의 global average pooling 계산 (weight 역할)\n",
    "    weights = torch.mean(grads, dim=(1, 2))\n",
    "\n",
    "    # weighted sum: 각 채널의 feature map에 weight를 곱해 합산\n",
    "    grad_cam_image = torch.zeros(fmap.shape[1:], dtype=torch.float32, device=fmap.device)\n",
    "    for i, w in enumerate(weights):\n",
    "        grad_cam_image += w * fmap[i, :, :]\n",
    "    grad_cam_image = grad_cam_image.cpu().numpy()\n",
    "\n",
    "    # ReLU 적용 및 정규화: 음수 값 제거 및 [0,1] 범위로 스케일링\n",
    "    grad_cam_image = np.maximum(grad_cam_image, 0)\n",
    "    grad_cam_image = (grad_cam_image - grad_cam_image.min()) / (grad_cam_image.max() - grad_cam_image.min() + 1e-8)\n",
    "\n",
    "    return grad_cam_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99341272",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grad_cam(models_list, image, ground_bbox, item, start_epochs, step_epochs):\n",
    "    layers = [\"layer1\", \"layer2\", \"layer3\", \"layer4\"]\n",
    "    num_epochs = len(models_list)\n",
    "    num_layers = len(layers)\n",
    "    \n",
    "    rows = num_epochs * 3 \n",
    "    cols = num_layers + 1 \n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(4 * cols, 4 * rows))\n",
    "    \n",
    "    orig_img = unnormalize(image)\n",
    "    ground_truth = [int(x) for x in ground_bbox]\n",
    "    \n",
    "    gt_only_img = visualize_bbox_on_image(orig_img.copy(), ground_truth) if 'visualize_bbox_on_image' in globals() else orig_img\n",
    "\n",
    "    for e_idx, model in enumerate(models_list):\n",
    "        epoch_num = start_epochs + (e_idx * step_epochs)\n",
    "        base_row = e_idx * 3\n",
    "        \n",
    "        axes[base_row, 0].imshow(orig_img)\n",
    "        axes[base_row, 0].set_title(f\"Ep {epoch_num}\\nOrig Image\")\n",
    "        axes[base_row, 0].axis(\"off\")\n",
    "        \n",
    "        axes[base_row + 1, 0].imshow(orig_img)\n",
    "        axes[base_row + 1, 0].set_title(f\"Ep {epoch_num}\\nOverlay Ref\")\n",
    "        axes[base_row + 1, 0].axis(\"off\")\n",
    "        \n",
    "        axes[base_row + 2, 0].imshow(gt_only_img)\n",
    "        axes[base_row + 2, 0].set_title(f\"Ep {epoch_num}\\nGround Truth\")\n",
    "        axes[base_row + 2, 0].axis(\"off\")\n",
    "\n",
    "        for l_idx, layer_name in enumerate(layers):\n",
    "            col_idx = l_idx + 1\n",
    "            \n",
    "            grad_cam = generate_grad_cam(model, item, activation_layer=layer_name)\n",
    "            grad_cam_resized = cv2.resize(grad_cam, (orig_img.shape[1], orig_img.shape[0]))\n",
    "            grad_cam_bbox = get_bbox(grad_cam_resized, threshold=0.5)\n",
    "            \n",
    "            axes[base_row, col_idx].imshow(grad_cam)\n",
    "            axes[base_row, col_idx].set_title(f\"Ep {epoch_num} {layer_name} CAM\")\n",
    "            axes[base_row, col_idx].axis(\"off\")\n",
    "            \n",
    "            axes[base_row + 1, col_idx].imshow(orig_img)\n",
    "            axes[base_row + 1, col_idx].imshow(grad_cam_resized, cmap='jet', alpha=0.5)\n",
    "            axes[base_row + 1, col_idx].set_title(f\"Ep {epoch_num} {layer_name} Overlay\")\n",
    "            axes[base_row + 1, col_idx].axis(\"off\")\n",
    "            \n",
    "            iou_score = get_iou(grad_cam_bbox, ground_truth)\n",
    "            grad_cam_img_bbox = visualize_both_bbox_on_image(orig_img.copy(), grad_cam_bbox, ground_truth)\n",
    "            \n",
    "            axes[base_row + 2, col_idx].imshow(grad_cam_img_bbox)\n",
    "            title_text = f\"{layer_name} IOU: {iou_score:.4f}\" if iou_score is not None else f\"{layer_name} Compare\"\n",
    "            axes[base_row + 2, col_idx].set_title(title_text)\n",
    "            axes[base_row + 2, col_idx].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d50bcad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 120\n",
      "Train samples: 12000\n",
      "Test samples: 8580\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# 데이터셋 경로 (재구성한 ImageFolder 형식)\n",
    "train_dir = os.path.join('stanford_dogs', 'train')\n",
    "test_dir = os.path.join('stanford_dogs', 'test')\n",
    "\n",
    "# Annotation 폴더 경로 (예: '/root/Annotation')\n",
    "annotation_dir = './Annotation'\n",
    "# 커스텀 데이터셋 생성: image, label, bbox 반환\n",
    "train_dataset = StanfordDogsDatasetWithBBox(root=train_dir, annotation_root=annotation_dir, transform=transform)\n",
    "valid_dataset = StanfordDogsDatasetWithBBox(root=test_dir, annotation_root=annotation_dir, transform=transform)\n",
    "\n",
    "batch_size = 12\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "num_classes = len(train_dataset.classes)\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(valid_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "069a80de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotation: \n",
      "\t\n",
      "folder: 02085620\n",
      "filename: n02085620_10074\n",
      "source: \n",
      "\t\t\n",
      "database: ImageNet database\n",
      "size: \n",
      "\t\t\n",
      "width: 333\n",
      "height: 500\n",
      "depth: 3\n",
      "segment: 0\n",
      "object: \n",
      "\t\t\n",
      "name: Chihuahua\n",
      "pose: Unspecified\n",
      "truncated: 0\n",
      "difficult: 0\n",
      "bndbox: \n",
      "\t\t\t\n",
      "xmin: 25\n",
      "ymin: 10\n",
      "xmax: 276\n",
      "ymax: 498\n"
     ]
    }
   ],
   "source": [
    "# Annotation 폴더에서 데이터를 골라보세요\n",
    "annotation_path = './Annotation/n02085620-Chihuahua/n02085620_10074'\n",
    "\n",
    "# XML 파일 파싱\n",
    "tree = ET.parse(annotation_path)\n",
    "root = tree.getroot()\n",
    "\n",
    "for elem in root.iter():\n",
    "    print(f\"{elem.tag}: {elem.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b5d0f9",
   "metadata": {},
   "source": [
    "### 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb3dd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, valid_loader, criterion, optimizer, num_epochs=2, model_type= ''):\n",
    "    # 결과 이미지를 저장할 폴더 생성\n",
    "    save_dir = './results'\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs} 시작\")\n",
    "\n",
    "        for batch_idx, (images, labels, _) in enumerate(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (preds == labels).sum().item()\n",
    "\n",
    "            if (batch_idx + 1) % 100 == 0:\n",
    "                print(f\"  Step {batch_idx+1} - Loss: {loss.item():.4f}\")\n",
    "\n",
    "        train_loss = running_loss / (batch_idx + 1)\n",
    "        train_acc = correct / total\n",
    "        print(f\"Train - Loss: {train_loss:.4f}, Accuracy: {train_acc*100:.2f}%\")\n",
    "\n",
    "        # 검증 단계\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "        all_probs = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels, _ in valid_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                running_loss += loss.item()\n",
    "                \n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                \n",
    "                # ROC AUC를 위해 Softmax로 확률값 계산\n",
    "                probs = F.softmax(outputs, dim=1)\n",
    "                \n",
    "                # GPU 텐서를 CPU로 옮기고 NumPy 배열로 변환하여 리스트에 저장\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "        # 전체 검증 데이터에 대한 지표 계산\n",
    "        test_loss = running_loss / len(valid_loader)\n",
    "        test_acc = correct / total\n",
    "        \n",
    "        # scikit-learn을 이용한 지표 산출 (다중 분류 기준으로 average='macro' 사용)\n",
    "        y_true = np.array(all_labels)\n",
    "        y_pred = np.array(all_preds)\n",
    "        y_prob = np.array(all_probs)\n",
    "        \n",
    "        val_precision = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "        val_recall = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "        val_f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "        val_conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "        \n",
    "        # ROC AUC (다중 분류 처리. 이진 분류일 경우 multi_class 옵션을 빼고 y_prob[:, 1]을 넣어야 할 수 있습니다)\n",
    "        try:\n",
    "            val_roc_auc = roc_auc_score(y_true, y_prob, multi_class='ovr')\n",
    "        except ValueError:\n",
    "            val_roc_auc = float('nan')\n",
    "            \n",
    "        print(f\"Validation - Loss: {test_loss:.4f}, Accuracy: {test_acc*100:.2f}%\")\n",
    "        print(f\"             Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1-score: {val_f1:.4f}\")\n",
    "        print(f\"             ROC AUC: {val_roc_auc:.4f}\")\n",
    "\n",
    "\n",
    "        plt.figure(figsize=(24, 20)) \n",
    "\n",
    "        sns.heatmap(val_conf_matrix, annot=False, fmt='d', cmap='Blues',\n",
    "                    xticklabels=5, yticklabels=5) \n",
    "\n",
    "        plt.title(f'Confusion Matrix (Large) - Epoch {epoch+1}', fontsize=20)\n",
    "        plt.xlabel('Predicted Label', fontsize=16)\n",
    "        plt.ylabel('True Label', fontsize=16)\n",
    "        plt.xticks(rotation=90, fontsize=12)\n",
    "        plt.yticks(rotation=0, fontsize=12)\n",
    "\n",
    "        cm_save_path = os.path.join(save_dir, f'confusion_matrix{model_type}_e{epoch+1}.png')\n",
    "        plt.savefig(cm_save_path, bbox_inches='tight', dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "        conf_matrix_errors = val_conf_matrix.copy()\n",
    "        np.fill_diagonal(conf_matrix_errors, 0)\n",
    "\n",
    "        plt.figure(figsize=(20, 16)) \n",
    "        sns.heatmap(conf_matrix_errors, annot=False, cmap='Reds', xticklabels=5, yticklabels=5)\n",
    "        \n",
    "        plt.title(f'Confusion Matrix (Errors Only) - Epoch {epoch+1}', fontsize=20)\n",
    "        plt.xlabel('Predicted Label', fontsize=16)\n",
    "        plt.ylabel('True Label', fontsize=16)\n",
    "        \n",
    "        cm_save_path = os.path.join(save_dir, f'confusion_matrix_errors{model_type}_e{epoch+1}.png')\n",
    "        plt.savefig(cm_save_path, bbox_inches='tight') # 잘리는 부분 없이 저장\n",
    "        plt.close()\n",
    "        # 모델 저장\n",
    "        cam_model_path = f'./models/cam_model{model_type}_e{epoch+1}.pt'\n",
    "        folder_path = os.path.dirname(cam_model_path)\n",
    "\n",
    "        if folder_path and not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "\n",
    "        torch.save(model, cam_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44031d46",
   "metadata": {},
   "source": [
    "### ResNet50  - pre trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d745538e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = models.resnet50(pretrained=True)\n",
    "model = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, len(train_dataset.classes))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5dc936e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=120, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b9caeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d3b577ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10 시작\n",
      "  Step 100 - Loss: 4.1755\n",
      "  Step 200 - Loss: 2.6337\n",
      "  Step 300 - Loss: 1.9698\n",
      "  Step 400 - Loss: 0.7333\n",
      "  Step 500 - Loss: 0.9808\n",
      "  Step 600 - Loss: 1.0784\n",
      "  Step 700 - Loss: 1.3584\n",
      "  Step 800 - Loss: 1.0656\n",
      "  Step 900 - Loss: 1.6203\n",
      "  Step 1000 - Loss: 0.7502\n",
      "Train - Loss: 1.8259, Accuracy: 57.47%\n",
      "Validation - Loss: 0.7468, Accuracy: 77.60%\n",
      "             Precision: 0.7806, Recall: 0.7722, F1-score: 0.7600\n",
      "             ROC AUC: 0.9967\n",
      "\n",
      "Epoch 2/10 시작\n",
      "  Step 100 - Loss: 0.7439\n",
      "  Step 200 - Loss: 0.6206\n",
      "  Step 300 - Loss: 1.0869\n",
      "  Step 400 - Loss: 0.2951\n",
      "  Step 500 - Loss: 0.3762\n",
      "  Step 600 - Loss: 0.6023\n",
      "  Step 700 - Loss: 0.4593\n",
      "  Step 800 - Loss: 0.7281\n",
      "  Step 900 - Loss: 0.5102\n",
      "  Step 1000 - Loss: 0.3111\n",
      "Train - Loss: 0.6039, Accuracy: 82.12%\n",
      "Validation - Loss: 0.6637, Accuracy: 79.46%\n",
      "             Precision: 0.8038, Recall: 0.7905, F1-score: 0.7853\n",
      "             ROC AUC: 0.9971\n",
      "\n",
      "Epoch 3/10 시작\n",
      "  Step 100 - Loss: 0.1952\n",
      "  Step 200 - Loss: 0.1132\n",
      "  Step 300 - Loss: 0.6793\n",
      "  Step 400 - Loss: 0.2916\n",
      "  Step 500 - Loss: 0.0372\n",
      "  Step 600 - Loss: 0.5602\n",
      "  Step 700 - Loss: 0.1523\n",
      "  Step 800 - Loss: 0.2346\n",
      "  Step 900 - Loss: 0.2424\n",
      "  Step 1000 - Loss: 0.3372\n",
      "Train - Loss: 0.3356, Accuracy: 90.11%\n",
      "Validation - Loss: 0.7439, Accuracy: 78.60%\n",
      "             Precision: 0.7911, Recall: 0.7782, F1-score: 0.7735\n",
      "             ROC AUC: 0.9962\n",
      "\n",
      "Epoch 4/10 시작\n",
      "  Step 100 - Loss: 0.1672\n",
      "  Step 200 - Loss: 0.3062\n",
      "  Step 300 - Loss: 0.1124\n",
      "  Step 400 - Loss: 0.3428\n",
      "  Step 500 - Loss: 0.0332\n",
      "  Step 600 - Loss: 0.1070\n",
      "  Step 700 - Loss: 0.1144\n",
      "  Step 800 - Loss: 0.4644\n",
      "  Step 900 - Loss: 0.6456\n",
      "  Step 1000 - Loss: 0.2499\n",
      "Train - Loss: 0.2363, Accuracy: 93.19%\n",
      "Validation - Loss: 0.8047, Accuracy: 78.10%\n",
      "             Precision: 0.7877, Recall: 0.7747, F1-score: 0.7707\n",
      "             ROC AUC: 0.9955\n",
      "\n",
      "Epoch 5/10 시작\n",
      "  Step 100 - Loss: 0.0576\n",
      "  Step 200 - Loss: 0.1020\n",
      "  Step 300 - Loss: 0.1267\n",
      "  Step 400 - Loss: 0.1264\n",
      "  Step 500 - Loss: 0.2971\n",
      "  Step 600 - Loss: 0.2830\n",
      "  Step 700 - Loss: 0.1401\n",
      "  Step 800 - Loss: 0.0337\n",
      "  Step 900 - Loss: 0.4189\n",
      "  Step 1000 - Loss: 0.2608\n",
      "Train - Loss: 0.1947, Accuracy: 93.96%\n",
      "Validation - Loss: 0.8332, Accuracy: 78.28%\n",
      "             Precision: 0.7848, Recall: 0.7799, F1-score: 0.7745\n",
      "             ROC AUC: 0.9952\n",
      "\n",
      "Epoch 6/10 시작\n",
      "  Step 100 - Loss: 0.1931\n",
      "  Step 200 - Loss: 0.0545\n",
      "  Step 300 - Loss: 0.2037\n",
      "  Step 400 - Loss: 0.0344\n",
      "  Step 500 - Loss: 0.0520\n",
      "  Step 600 - Loss: 0.0940\n",
      "  Step 700 - Loss: 0.4419\n",
      "  Step 800 - Loss: 0.0467\n",
      "  Step 900 - Loss: 0.0787\n",
      "  Step 1000 - Loss: 0.1967\n",
      "Train - Loss: 0.1451, Accuracy: 96.02%\n",
      "Validation - Loss: 0.8955, Accuracy: 77.16%\n",
      "             Precision: 0.7745, Recall: 0.7673, F1-score: 0.7616\n",
      "             ROC AUC: 0.9941\n",
      "\n",
      "Epoch 7/10 시작\n",
      "  Step 100 - Loss: 0.0534\n",
      "  Step 200 - Loss: 0.3384\n",
      "  Step 300 - Loss: 0.1393\n",
      "  Step 400 - Loss: 0.0138\n",
      "  Step 500 - Loss: 0.0112\n",
      "  Step 600 - Loss: 0.2033\n",
      "  Step 700 - Loss: 0.0260\n",
      "  Step 800 - Loss: 0.1348\n",
      "  Step 900 - Loss: 0.0135\n",
      "  Step 1000 - Loss: 0.0650\n",
      "Train - Loss: 0.1356, Accuracy: 96.15%\n",
      "Validation - Loss: 1.0048, Accuracy: 75.85%\n",
      "             Precision: 0.7616, Recall: 0.7596, F1-score: 0.7495\n",
      "             ROC AUC: 0.9930\n",
      "\n",
      "Epoch 8/10 시작\n",
      "  Step 100 - Loss: 0.3994\n",
      "  Step 200 - Loss: 0.0772\n",
      "  Step 300 - Loss: 0.0471\n",
      "  Step 400 - Loss: 0.0587\n",
      "  Step 500 - Loss: 0.0071\n",
      "  Step 600 - Loss: 0.2670\n",
      "  Step 700 - Loss: 0.1288\n",
      "  Step 800 - Loss: 0.1740\n",
      "  Step 900 - Loss: 0.1615\n",
      "  Step 1000 - Loss: 0.5668\n",
      "Train - Loss: 0.1170, Accuracy: 96.42%\n",
      "Validation - Loss: 0.9957, Accuracy: 76.52%\n",
      "             Precision: 0.7688, Recall: 0.7593, F1-score: 0.7545\n",
      "             ROC AUC: 0.9928\n",
      "\n",
      "Epoch 9/10 시작\n",
      "  Step 100 - Loss: 0.0145\n",
      "  Step 200 - Loss: 0.0878\n",
      "  Step 300 - Loss: 0.2799\n",
      "  Step 400 - Loss: 0.1832\n",
      "  Step 500 - Loss: 0.0071\n",
      "  Step 600 - Loss: 0.0062\n",
      "  Step 700 - Loss: 0.0989\n",
      "  Step 800 - Loss: 0.0169\n",
      "  Step 900 - Loss: 0.0466\n",
      "  Step 1000 - Loss: 0.0176\n",
      "Train - Loss: 0.1163, Accuracy: 96.75%\n",
      "Validation - Loss: 1.0684, Accuracy: 75.34%\n",
      "             Precision: 0.7596, Recall: 0.7534, F1-score: 0.7470\n",
      "             ROC AUC: 0.9918\n",
      "\n",
      "Epoch 10/10 시작\n",
      "  Step 100 - Loss: 0.0961\n",
      "  Step 200 - Loss: 0.0073\n",
      "  Step 300 - Loss: 0.0136\n",
      "  Step 400 - Loss: 0.1737\n",
      "  Step 500 - Loss: 0.0212\n",
      "  Step 600 - Loss: 0.0181\n",
      "  Step 700 - Loss: 0.1280\n",
      "  Step 800 - Loss: 0.0214\n",
      "  Step 900 - Loss: 0.0048\n",
      "  Step 1000 - Loss: 0.0719\n",
      "Train - Loss: 0.0950, Accuracy: 97.22%\n",
      "Validation - Loss: 1.1028, Accuracy: 74.44%\n",
      "             Precision: 0.7506, Recall: 0.7393, F1-score: 0.7346\n",
      "             ROC AUC: 0.9912\n",
      "CPU times: user 17min 53s, sys: 1min 32s, total: 19min 25s\n",
      "Wall time: 19min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_model(model, train_loader, valid_loader, criterion, optimizer, num_epochs=max_epochs, model_type = \"_pre\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfba572f",
   "metadata": {},
   "source": [
    "### CAM, Grad CAM 체크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d24a8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epochs = 1\n",
    "step_epochs = 3\n",
    "\n",
    "models_list = []\n",
    "\n",
    "for epoch in range(start_epochs, max_epochs + 1, step_epochs):\n",
    "    model_path = f'./models/cam_model_pre_e{epoch}.pt'\n",
    "    \n",
    "    model = torch.load(model_path,  map_location=device, weights_only=False)\n",
    "    model.eval() \n",
    "    \n",
    "    models_list.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7fb8e9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation dataset 확인용\n",
    "def show_dataset_grid(dataset, start_index=0, num_images=12, cols=4):\n",
    "    \"\"\"데이터셋의 이미지를 역정규화하여 격자 형태로 시각화합니다.\"\"\"\n",
    "    if start_index + num_images > len(dataset):\n",
    "        num_images = len(dataset) - start_index\n",
    "\n",
    "    rows = (num_images + cols - 1) // cols\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(4 * cols, 4 * rows))\n",
    "    axes = np.atleast_1d(axes).flatten()\n",
    "\n",
    "    for i in range(num_images):\n",
    "        current_idx = start_index + i\n",
    "        image_tensor, label, _ = dataset[current_idx]\n",
    "\n",
    "        img_to_show = unnormalize(image_tensor)\n",
    "        \n",
    "        axes[i].imshow(img_to_show)\n",
    "        axes[i].set_title(f\"Idx: {current_idx} | Lbl: {label}\")\n",
    "        axes[i].axis(\"off\")\n",
    "\n",
    "    # 남은 빈 칸 제거\n",
    "    for j in range(num_images, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# show_dataset_grid(valid_dataset, start_index=0, num_images=500, cols=10)\n",
    "# show_dataset_grid(valid_dataset, start_index=501, num_images=500, cols=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8fbc9b",
   "metadata": {},
   "source": [
    "### 단일 객체일 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4bbe773f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label, ground_bbox = valid_dataset[0]\n",
    "item = image.unsqueeze(0).to(device)\n",
    "orig_img = unnormalize(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6cd715",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_cam(models_list, image, ground_bbox, item, start_epochs, step_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7250d465",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_grad_cam(models_list, image, ground_bbox, item, start_epochs, step_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb953a2e",
   "metadata": {},
   "source": [
    "### 여러 객체가 있을 때"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6c58a4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label, ground_bbox = valid_dataset[326]\n",
    "item = image.unsqueeze(0).to(device)\n",
    "orig_img = unnormalize(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49de9992",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_cam(models_list, image, ground_bbox, item, start_epochs, step_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf84a83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_grad_cam(models_list, image, ground_bbox, item, start_epochs, step_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e0009b",
   "metadata": {},
   "source": [
    "### 객체가 멀리 있을 때"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d72c62af",
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label, ground_bbox = valid_dataset[544]\n",
    "# image, label, ground_bbox = valid_dataset[292]\n",
    "item = image.unsqueeze(0).to(device)\n",
    "orig_img = unnormalize(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a58cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_cam(models_list, image, ground_bbox, item, start_epochs, step_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873d0547",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_grad_cam(models_list, image, ground_bbox, item, start_epochs, step_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c258cc",
   "metadata": {},
   "source": [
    "### 이미지를 출력한 상태로 파일을 업로드시 주피터 파일 용량이 너무 크기 때문에 변경\n",
    "### 1.  단일객체\n",
    "[CAM 단일 객체](./img/cam_1.png)  \n",
    "[Grad CAM 단일 객체](./img/grad_cam_1.png)  \n",
    "\n",
    "### 2. 여러 객체\n",
    "[CAM 여러 객체](./img/cam_2.png)  \n",
    "[Grad 여러 단일 객체](./img/grad_cam_2.png)\n",
    "\n",
    "### 3. 먼 거리 객체\n",
    "[CAM 먼 거리 객체](./img/cam_3.png)  \n",
    "[Grad CAM 먼 거리 객체](./img/grad_cam_3.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
